{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/sam/All-Program/App/deep-reinforcement-learning/')\n",
    "\n",
    "from DeepRL.src import commons as cmn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(scores, loss, td_error):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(35,8))\n",
    "    ax = ax.ravel()\n",
    "    ax[0].plot(scores)\n",
    "    ax[0].set_title('scores')\n",
    "    ax[1].plot(loss)\n",
    "    ax[1].set_title('loss')\n",
    "    ax[2].plot(td_error)\n",
    "    ax[2].set_title('td_error')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1:\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Config File ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 0\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Initializing Replay Buffer .... .... ....\n",
      "[INIT] Initializing RLAgent .... .... ....\n",
      "[INIT] Initializing Visual QNetwork (net1) .... .... ....\n",
      "[INIT] Initializing Visual QNetwork (net1) .... .... ....\n",
      "Episode 100\tAverage Score: -0.04\n",
      "Episode 186\tAverage Score: 0.00"
     ]
    }
   ],
   "source": [
    "from main import CollectBanana, DDQN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Config:\n",
    "    print('Reading Config File ............')\n",
    "    MODEL_NAME = 'model_1'\n",
    "    \n",
    "    # ENVIRONMEMT PARAMETER\n",
    "    STATE_SIZE = 37\n",
    "    ACTION_SIZE = 4\n",
    "    NUM_EPISODES = 2000\n",
    "    NUM_TIMESTEPS = 1000\n",
    "    \n",
    "    # MODEL PARAMETERS\n",
    "    SEED = 0\n",
    "    BUFFER_SIZE = int(1e05)\n",
    "    BATCH_SIZE = 64\n",
    "    UPDATE_AFTER_STEP = 4\n",
    "    \n",
    "    # RL LEARNING PARAMETERS\n",
    "    SOFT_UPDATE = True\n",
    "    TAU = 0.001  # Soft update parameter for target_network\n",
    "    TAU_DECAY = 0.003\n",
    "    TAU_MIN = 0.05\n",
    "    DECAY_TAU = False\n",
    "    \n",
    "    HARD_UPDATE = False\n",
    "    HARD_UPDATE_FREQUENCY = 30\n",
    "    \n",
    "    GAMMA = 0.99  # Discount value\n",
    "    EPSILON = 1  # Epsilon value for action selection\n",
    "    EPSILON_DECAY = 0.995  # Epsilon decay for epsilon greedy policy\n",
    "    EPSILON_MIN = 0.01  # Minimum epsilon to reach\n",
    "    \n",
    "    # ML LEARNING PARAMETERS\n",
    "    LEARNING_RATE = 0.0005  # Learning rate for the network\n",
    "    LEARNING_RATE_DECAY = None\n",
    "    \n",
    "    Q_LEARNING_TYPE = 'dqn' # dqn also available, ddqn is double dqn\n",
    "    NET_NAME = 'net1'\n",
    "    \n",
    "    # PATHS\n",
    "    STATS_JSON_PATH = '/Users/sam/All-Program/App-DataSet/DeepRL/collect_banana_visual/stats/%s_stats.json'%str(MODEL_NAME)\n",
    "    CHECKPOINT_PATH = '/Users/sam/All-Program/App-DataSet/DeepRL/collect_banana_visual/checkpoints/%s.pth'%str(MODEL_NAME)\n",
    "    \n",
    "    if DECAY_TAU and TAU <= TAU_MIN:\n",
    "        raise ValueError(\"TAU should have larger value that TAU_MIN\")\n",
    "        \n",
    "    if EPSILON <= EPSILON_MIN:\n",
    "        raise ValueError(\"EPSILON should have larger value that EPSILON_MIN\")\n",
    "    \n",
    "    if SOFT_UPDATE and HARD_UPDATE:\n",
    "        raise ValueError('Only one of soft_update and hard_update can be active')\n",
    "        \n",
    "    if not os.path.exists(os.path.dirname(STATS_JSON_PATH)):\n",
    "        os.makedirs(os.path.dirname(STATS_JSON_PATH))\n",
    "        \n",
    "    if not os.path.exists(os.path.dirname(CHECKPOINT_PATH)):\n",
    "        os.makedirs(os.path.dirname(CHECKPOINT_PATH))\n",
    "\n",
    "    \n",
    "env = CollectBanana(env_type='visual')\n",
    "dqn = DDQN(Config, env, env_type='visual')\n",
    "env.train_mode = True\n",
    "scores = dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
